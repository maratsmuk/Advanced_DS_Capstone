{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[Zero-crossings_in_time_series].import_libraries.python.ipynb","provenance":[],"authorship_tag":"ABX9TyMBGi4BvhSD9ZyZ+7RtZB6N"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"b9OzyLQaWJJK"},"source":["# Efficient determination of zero-crossings in noisy real-life time series\r\n","## Advanced Data Science Capstone Project\r\n","### Import of all necessary libraries\r\n","\r\n","The following main libraries are imported:\r\n","\r\n","1. Scikit-learn\r\n","2. Pyspark\r\n","3. Tensorflow and Keras\r\n","4. Numpy\r\n","\r\n","In each step, particular libraries are used:\r\n","\r\n","1. Initial data exploration: numpy, matplotlib.pyplot, random, math.\r\n","2. ETL: pyspark (with SparkConf, SparkContext, SparkSession, ml.linalg.Vectors) is added.\r\n","3. Feature engineering: pyspark (with ml.feature.PolynomialExpansion, ml.feature.MinMaxScaler), Scikit-learn (with preprocessing.PolynomialFeatures and preprocessing.MinMaxScaler) are added.\r\n","4. Model definition: pyspark.ml.regression.LinearRegression,  sklearn.linear_model.LinearRegression, keras.models.Sequential and keras.models.Dense are added.\r\n","5. Model training: sklearn.metrics.r2_score and keras.callbacks.EarlyStopping are added.\r\n","5. Model evaluation: No additional libraries are required.\r\n","6. Model deployment: os and warnings can be added (optional)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sayv528JV2RQ","executionInfo":{"status":"ok","timestamp":1614545374538,"user_tz":-60,"elapsed":48022,"user":{"displayName":"Marat Mukhametzhanov","photoUrl":"","userId":"00296603522146618027"}},"outputId":"3166a22a-8132-4606-d64b-63b3605bf85f"},"source":["import random\r\n","\r\n","import sklearn\r\n","from sklearn.linear_model import LinearRegression\r\n","from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\r\n","from sklearn.metrics import r2_score\r\n","\r\n","try:\r\n","  import pyspark\r\n","except:\r\n","  !pip install pyspark\r\n","  import pyspark\r\n","from pyspark import SparkConf, SparkContext\r\n","sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\r\n","from pyspark.sql import SparkSession\r\n","spark = SparkSession \\\r\n","    .builder \\\r\n","    .getOrCreate()\r\n","\r\n","from pyspark.ml.feature import PolynomialExpansion\r\n","from pyspark.ml.linalg import Vectors\r\n","from pyspark.ml.feature import MinMaxScaler as sparkScaler\r\n","from pyspark.ml.regression import LinearRegression as spark_LR\r\n","from pyspark.ml import Pipeline, PipelineModel\r\n","\r\n","\r\n","import tensorflow as tf\r\n","from tensorflow.keras.models import Sequential\r\n","from tensorflow.keras.layers import Dense\r\n","from tensorflow.keras import optimizers\r\n","from keras.callbacks import EarlyStopping\r\n","\r\n","import warnings\r\n","import os\r\n","\r\n","import numpy as np\r\n","import matplotlib.pyplot as plt\r\n","import math\r\n","from math import exp, sin, cos, log, pi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pyspark\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/67/5158f846202d7f012d1c9ca21c3549a58fd3c6707ae8ee823adcaca6473c/pyspark-3.0.2.tar.gz (204.8MB)\n","\u001b[K     |████████████████████████████████| 204.8MB 73kB/s \n","\u001b[?25hCollecting py4j==0.10.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n","\u001b[K     |████████████████████████████████| 204kB 21.6MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186687 sha256=93fd8b1a47a651085060ecd9e52f3a48184471f41469f86b30b48511587d4d45\n","  Stored in directory: /root/.cache/pip/wheels/8b/09/da/c1f2859bcc86375dc972c5b6af4881b3603269bcc4c9be5d16\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9 pyspark-3.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MkGpZtLfWHOP"},"source":[""],"execution_count":null,"outputs":[]}]}