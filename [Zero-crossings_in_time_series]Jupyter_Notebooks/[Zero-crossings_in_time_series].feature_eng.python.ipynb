{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[Zero-crossings_in_time_series].feature_eng.python.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMNqurGN0d5uZJt020lPscU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"aqMH7JuP-cj1"},"source":["# Efficient determination of zero-crossings in noisy real-life time series\r\n","## Advanced Data Science Capstone Project\r\n","### Feature creation.\r\n","In this notebook, the features are created from the array of the time variable $t$. The presented functions are called during the simulation every time, when the Machine or Deep learning model is re-fitted. \r\n","\r\n"]},{"cell_type":"markdown","metadata":{"id":"ExCYuByLEm_i"},"source":["First, let us import all necessary libraries. "]},{"cell_type":"code","metadata":{"id":"ZhIN_QGt-aTw"},"source":["#Here, the path to the file [Zero_crossings_in_time_series]_import_libraries_python.ipynb should be indicated.\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iQ_tXj0ZE3sD"},"source":["Then, let us prepare our features. Here, `data` is the vector or Spark dataframe (depending on the `model_context` variable: if `model_context=1`, then the Spark dataframes are used, otherwise, standard numpy arrays are used). \r\n","`N_train` and `N_predict` are the number of observations for the train and predictions sets, respectively, while `degrees` is the number of degrees for the polynomial features. The output of this function is two sets of observations (for training and prediction, respectively) in the same format as the input `data`. \r\n","\r\n","First, the whole input is transformed into polynomial features using Pyspark or Scikit-learn. Then, the polynomial features are scaled into the interval $[0,1]$. This normalization is required, since otherwise the features of higher degrees and the features of smaller degrees will have very different impact on the result: e.g., if `degrees = 10` and $t_1 = 10$, then $t_1^{10} = 10^{10} = 10000000000 >> t_1^1 = 10$. From the other hand, if $t_2 = 0.1$, then $t_2^{10} = 10^{-10} = 0.0000000001 << t_2^1 = 0.1$."]},{"cell_type":"code","metadata":{"id":"HrRuuVByEyJd","executionInfo":{"status":"ok","timestamp":1614881113235,"user_tz":-60,"elapsed":858,"user":{"displayName":"Marat Mukhametzhanov","photoUrl":"","userId":"00296603522146618027"}}},"source":["def prepare_features(data, N_train,N_predict,degrees, model_context=0):\r\n","  if model_context==1:\r\n","    polyExpansion = PolynomialExpansion(degree=degrees, inputCol=\"t\", outputCol=\"t_poly\",)\r\n","    polyDF = polyExpansion.transform(data)\r\n","    scaledDF=sparkScaler(inputCol=\"t_poly\", outputCol=\"features\").fit(polyDF).transform(polyDF)\r\n","    scaledDF=scaledDF.drop('t_poly')\r\n","    scaledDF.createOrReplaceTempView(\"scaledDF\")\r\n","    train_t = spark.sql(\"select * from scaledDF limit \"+str(N_train))\r\n","    predict_t = spark.sql(\"select * from (select * from scaledDF order by t desc limit \"+str(N_predict)+\") order by t\")\r\n","  else:\r\n","    if model_context==0:\r\n","      t_poly = PolynomialFeatures(degrees,include_bias=False).fit_transform(data[:,np.newaxis])\r\n","      t_scaled = MinMaxScaler().fit_transform(t_poly)\r\n","    else:\r\n","      t_poly = PolynomialFeatures(degrees,include_bias=False).fit_transform(data[:,np.newaxis])\r\n","      t_scaled = MinMaxScaler().fit_transform(t_poly)\r\n","      #t_scaled[:,0] = np.ones(t_scaled.shape[0])\r\n","    train_t = t_scaled[0:N_train]\r\n","    predict_t = t_scaled[N_train:N_train+N_predict]\r\n","  return train_t,predict_t"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HriTGkCYHGJw"},"source":["Just an example of the features engineering result."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":490},"id":"uxBk5b6VHEdw","executionInfo":{"status":"ok","timestamp":1614789734672,"user_tz":-60,"elapsed":3987,"user":{"displayName":"Marat Mukhametzhanov","photoUrl":"","userId":"00296603522146618027"}},"outputId":"4ff1fc8e-0d8f-46b6-d2d5-95eb61dab788"},"source":["print(\"Just an example of the features engineering result\")\r\n","#Create any time array and define the number of training and test sets N_1 and N_2:\r\n","t = np.arange(0.1,1.01,0.2)\r\n","N = len(t)\r\n","N_1 = int(N*0.6)\r\n","N_2 = N-N_1\r\n","print(N,N_1,N_2)\r\n","#Just an example using degrees = 2:\r\n","degrees = 2\r\n","#Prepare the features using simple, not Spark, data:\r\n","train_t,predict_t = prepare_features(t,N_1,N_2,degrees,0)\r\n","print(train_t)\r\n","print(predict_t)\r\n","\r\n","#Prepare the features for Spark data:\r\n","#First, transform the data into a Spark dataframe \r\n","#(in practice, it is done automatically by the provide_new_data procedure):\r\n","t_list = []\r\n","y_list = []\r\n","for t_val in t:\r\n","  t_list.append(Vectors.dense(t_val))\r\n","  y_list.append(np.nan)\r\n","#The values of the dependent variable x are fixed equal to Nan just for simplicity,\r\n","#since at this step they are not required (they are fixed automatically by \r\n","#the provide_new_data procedure):\r\n","t_df = spark.createDataFrame(sc.parallelize(zip(t_list,y_list)),[\"t\",\"x\"])\r\n","train_t,predict_t = prepare_features(t_df,N_1,N_2,degrees,1)\r\n","train_t.show()\r\n","predict_t.show()\r\n","train_t.createOrReplaceTempView(\"train_t\")\r\n","predict_t.createOrReplaceTempView(\"predict_t\")\r\n","display(spark.sql(\"select features from train_t\").rdd.map(lambda row: row.features.toArray().tolist()).collect())\r\n","display(spark.sql(\"select features from predict_t\").rdd.map(lambda row: row.features.toArray().tolist()).collect())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Just an example of the features engineering result\n","5 3 2\n","[[0.   0.  ]\n"," [0.25 0.1 ]\n"," [0.5  0.3 ]]\n","[[0.75 0.6 ]\n"," [1.   1.  ]]\n","+--------------------+---+--------------------+\n","|                   t|  x|            features|\n","+--------------------+---+--------------------+\n","|               [0.1]|NaN|           (2,[],[])|\n","|[0.30000000000000...|NaN|[0.25,0.099999999...|\n","|[0.5000000000000001]|NaN|[0.50000000000000...|\n","+--------------------+---+--------------------+\n","\n","+--------------------+---+--------------------+\n","|                   t|  x|            features|\n","+--------------------+---+--------------------+\n","|[0.7000000000000001]|NaN|[0.75,0.599999999...|\n","|[0.9000000000000001]|NaN|           [1.0,1.0]|\n","+--------------------+---+--------------------+\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["[[0.0, 0.0],\n"," [0.25, 0.09999999999999998],\n"," [0.5000000000000001, 0.30000000000000004]]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["[[0.75, 0.5999999999999999], [1.0, 1.0]]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"6m8y6xJEHUrI"},"source":[""],"execution_count":null,"outputs":[]}]}