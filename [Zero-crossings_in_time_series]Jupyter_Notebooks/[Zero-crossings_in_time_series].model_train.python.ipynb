{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[Zero-crossings_in_time_series].model_train.python.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMloWR4dMgVyv6qH8rQPRHe"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9Ser1lIJSVUI"},"source":["# Efficient determination of zero-crossings in noisy real-life time series\r\n","## Advanced Data Science Capstone Project\r\n","### Model training.\r\n","In this notebook, the compiled models are trained. The presented functions are called every time, when a new data is received and the models should be re-fitted.\r\n","\r\n","First, all necessary libraries are imported.\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"pMdrKqiweZHB"},"source":["#Here, the path to the file [Zero_crossings_in_time_series]_import_libraries_python.ipynb should be indicated.\r\n","try:\r\n","  %run /content/[Zero_crossings_in_time_series]_import_libraries_python.ipynb\r\n","except:\r\n","  None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coZgiyjgSvXx"},"source":["Then, the model compiled at the previous step is fitted. Here, if the simple linear regression model is used, then we need only the features and the values of the objective function given in two numpy arrays data1 and data2, respectively. Otherwise, if the Spark models are used, then all the data should be given in a unique dataframe data1 (data2 is not used in this case and, e.g., `data2=None` can be passes as the input parameter to this function). Finally, if DNNs are used, then `data1` and `data2` are again two numpy arrays of the respective dimensions. 500 epochs are used with the batches of the size N/10, where N is the number of samples in the training set. Finally, EarlyStopping is applied, when the loss function is not improved during 5 consecutive epochs in order to avoid overfitting. "]},{"cell_type":"code","metadata":{"id":"Or9DsUb1X8l4"},"source":["#############################################\n","def fit_model(model, data1, data2, model_context = 0):\n","    if model_context==0:\n","      model.fit(data1, data2)\n","    elif model_context==1:\n","      model = model.fit(data1)\n","    else: \n","      n_epochs = 500\n","      batch_size=int(data1.shape[0]/10)\n","      es = EarlyStopping(monitor='loss', mode='min', verbose=0, patience=5)\n","      model.fit(data1, data2, epochs=n_epochs, batch_size=batch_size, verbose=0,callbacks=[es])\n","    return model\n","#############################################\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LAYUY41uTup4"},"source":["The `predict_model` procedure returns the list of the predicted values at the time steps given in the `data` input parameter. If `model_context=1`, i.e., Spark is used, then `data` is a Spark dataframe of the polynomial features transformed at the previous stages. Otherwise, `data` is a simple numpy array transformed into polynomial features from the previous stages."]},{"cell_type":"code","metadata":{"id":"wP3t_aezOKru"},"source":["#############################################\r\n","def predict_model(model,data,model_context = 0):\r\n","  if model_context==1:\r\n","    df_predicted = model.transform(data)\r\n","    df_predicted.createOrReplaceTempView(\"df_predicted\")\r\n","    y_predicted = spark.sql(\"select * from df_predicted\").rdd.map(lambda row: row.prediction).collect()\r\n","  else: \r\n","    y_predicted = model.predict(data)\r\n","  return y_predicted\r\n","#############################################\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-SdKIn9gUMkz"},"source":["`find_degrees` is, probably, the most complicated part of this step. It returns the optimal number of polynomial degrees for the selected model in the feature engineering. Here, `data1` and `data2` are two numpy arrays of the time vector $t$ and the observed values $g(t)$ (if `model_context` is different from 1) and a Spark dataframe (`data1`) and any other object, e.g., `None` (if `model_context = 1`). `N_train` and `N_predict` are the numbers of samples for the training and evaluating, respectively (`N_train+N_predict` should be equal to the size of the samples in `data1`). \r\n","\r\n","For each degree from 1 to 30, the respective model is compiled and fitted using N_train values from data1 (and data2, eventually). After that, the last N_predict values from data1 are predicted by the fitted model. The R2 value is calculated using the respective Scikit-learn's procedure. The number of degrees is selected maximizing the R2 value. "]},{"cell_type":"code","metadata":{"id":"to21enXnORYa"},"source":["#############################################\r\n","def find_degrees(data1,data2,N_train, N_predict, model_context):\r\n","    R2_values_test = np.zeros(30)\r\n","    if model_context==1:\r\n","      data1.createOrReplaceTempView(\"data1\")\r\n","      y_real = np.array(spark.sql(\"select * from data1\").rdd.map(lambda row: row.x).collect())\r\n","    else:\r\n","      y_real = data2\r\n","    for deg in range(1,31):\r\n","      if model_context==2:\r\n","        inp_shape = deg\r\n","      else:\r\n","        inp_shape=0\r\n","      train_t,predict_t = prepare_features(data1,N_train,N_predict,deg,model_context)\r\n","      model = compile_model(model_context,inp_shape)\r\n","      if model_context==1:\r\n","        model = fit_model(model,train_t,None,model_context)  \r\n","      else:\r\n","        model = fit_model(model,train_t,data2[0:N_train],model_context)\r\n","      y_predicted = predict_model(model,predict_t,model_context)\r\n","      y_predicted=np.array(y_predicted).reshape(len(y_predicted))\r\n","      R2_values_test[deg-1] = r2_score(y_real[N_train:N_train+N_predict], y_predicted)\r\n","    degrees = np.argmax(R2_values_test)+1 \r\n","    #print(R2_values_test)\r\n","    print('Optimal degree number is: '+str(degrees)+'\\nR2-test with '+str(degrees)+' degrees is '+str(R2_values_test[degrees-1]))\r\n","    return degrees\r\n","#############################################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKhRIuJGcIsC"},"source":[""],"execution_count":null,"outputs":[]}]}